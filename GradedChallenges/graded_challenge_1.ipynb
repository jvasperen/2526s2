{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "scenic-agent",
   "metadata": {},
   "source": [
    "# Graded Challenge 1 (DA1 and DA2)\n",
    "\n",
    "Now that we've seen how Python works and and how to use Pandas, it's time for you to combine and apply this knowledge to analyse social media data.\n",
    "\n",
    "The warm-up challenge consists of two components:\n",
    "1. **Programming**\n",
    "2. **Interpretation**\n",
    "\n",
    "**Some important notes for the challenges:**\n",
    "1. While we of course like when you get all the answers right, the important thing is to exercise and apply the knowledge. So we will give some credit for challenges that may not be complete, as long as we see enough effort. The rubric (see Canvas) reflects this.\n",
    "2. You will deliver the challenges on Canvas. Make sure to follow the turning-in instructions. \n",
    "\n",
    "### Usage of AI-generated content\n",
    "\n",
    "In line with the course policies, it is **not allowed** to use AI-generated content to any explanations or text provided in MarkDown. This means that for **Interpretation** students are **not allowed** to use any AI-generated content. \n",
    "\n",
    "For **Programming**, however, AI-assisted tools may be helpful to understand how to solve some of the questions - as one may also use Google to search for solutions online (e.g., on StackOverflow or other websites). They can, therefore, be used for the programming part of this challenge exceptionally - and only for this part - unless otherwise noted in the question. It is however your responsibility to test and make sure that the solution works - and explain this, in your own words, in the interpretation section.\n",
    "\n",
    "### Facing issues? \n",
    "\n",
    "We are constantly monitoring the issues on the GitHub to help you out. Don't hesitate to log an issue there, explaining well what the problem is, showing the code you are using, and the error message you may be receiving. \n",
    "\n",
    "**Important:** We are only monitoring the repository in weekdays, from 9.30 to 17.00. Issues logged after this time will most likely be answered the next day. This means you should not wait for our response before submitting a challenge :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-belly",
   "metadata": {},
   "source": [
    "## 1. Programming\n",
    "\n",
    "Select one of the datasets available for this class. Load that file using ```pandas``` and answer the following questions:\n",
    "1. How many cases are in your dataset?\n",
    "2. What columns does the dataset contain and what is their data type?\n",
    "3. Are there any missing values?\n",
    "4. What is the lowest number, highest number, average and the standard deviation of engagement (you can choose your metric)?\n",
    "5. What is the most common language of the entries?\n",
    "6. Who are the 5 most frequent authors in your dataset (can be poster, advertiser or subreddit)?\n",
    "7. Write a function that categorizes the content of your dataset based on information available in one of the columns (e.g., engagement, some characteristics,  or any other column). It's up to you to define what this function should do (and explain it). One possibility is to create a function that returns 1 if the row belongs to a certain category, and 0 if it does not. You can be more creative than this (see Rubric on Canvas). *This week, we ask you to write the function and test if it works (following the tutorial). You will learn how to apply it to the dataframe next week.*\n",
    "\n",
    "**Note on question 7.** We do not expect you to use  ```text``` column for the function. We will learn how to categorize that column in the next tutorials. For question 7 you are **not** allowed to use AI-assisted tools.\n",
    "\n",
    "### Using MarkDown\n",
    "\n",
    "Make sure to combine code and markdown to answer these questions. Mention specifically the question (and question number) and the answer in markdown, relating to the code and the output of the code. Failing to do will impact the grade, as we will not be able to see whether you answered the question.\n",
    "\n",
    "*Wordcount: There is no maximum wordcount for this section.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc4b04f-8dd1-4b35-ab55-8dcba159aab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>author_id</th>\n",
       "      <th>reply_settings</th>\n",
       "      <th>edit_history_tweet_ids</th>\n",
       "      <th>referenced_tweets</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>...</th>\n",
       "      <th>entities.annotations</th>\n",
       "      <th>entities.urls</th>\n",
       "      <th>attachments.media_keys</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>entities.cashtags</th>\n",
       "      <th>entities.hashtags</th>\n",
       "      <th>geo.place_id</th>\n",
       "      <th>attachments.poll_ids</th>\n",
       "      <th>geo.coordinates.type</th>\n",
       "      <th>geo.coordinates.coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-07T15:29:33.000Z</td>\n",
       "      <td>1622980893416517632</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>1547500137026551809</td>\n",
       "      <td>everyone</td>\n",
       "      <td>[1622980893416517632]</td>\n",
       "      <td>[{'type': 'retweeted', 'id': '1622902416868884...</td>\n",
       "      <td>1622980893416517632</td>\n",
       "      <td>RT @SaulStaniforth: \"I'm paid about £13 an hou...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-07T15:29:08.000Z</td>\n",
       "      <td>1622980790761013255</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>1015653575916126211</td>\n",
       "      <td>everyone</td>\n",
       "      <td>[1622980790761013255]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1622980790761013255</td>\n",
       "      <td>Trade: Brian Windhorst is sending $11.36 to a ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'start': 7, 'end': 21, 'probability': 0.8675...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-07T15:29:08.000Z</td>\n",
       "      <td>1622980787946635264</td>\n",
       "      <td>de</td>\n",
       "      <td>False</td>\n",
       "      <td>1356299130994388994</td>\n",
       "      <td>everyone</td>\n",
       "      <td>[1622980787946635264]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1622980787946635264</td>\n",
       "      <td>Falschparker Thread ist neue McDonalds Thread....</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-07T15:29:07.000Z</td>\n",
       "      <td>1622980784234840064</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>2989505299</td>\n",
       "      <td>everyone</td>\n",
       "      <td>[1622980784234840064]</td>\n",
       "      <td>[{'type': 'retweeted', 'id': '1622721798902251...</td>\n",
       "      <td>1622980784234840064</td>\n",
       "      <td>RT @weirdlilguys: u got mcdonalds money? https...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'start': 24, 'end': 32, 'probability': 0.612...</td>\n",
       "      <td>[{'start': 41, 'end': 64, 'url': 'https://t.co...</td>\n",
       "      <td>[3_1622464177188093952]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-07T15:29:05.000Z</td>\n",
       "      <td>1622980778622857216</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>859693365058617344</td>\n",
       "      <td>everyone</td>\n",
       "      <td>[1622980778622857216]</td>\n",
       "      <td>[{'type': 'retweeted', 'id': '1622721798902251...</td>\n",
       "      <td>1622980778622857216</td>\n",
       "      <td>RT @weirdlilguys: u got mcdonalds money? https...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'start': 24, 'end': 32, 'probability': 0.612...</td>\n",
       "      <td>[{'start': 41, 'end': 64, 'url': 'https://t.co...</td>\n",
       "      <td>[3_1622464177188093952]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at      conversation_id lang  possibly_sensitive  \\\n",
       "0  2023-02-07T15:29:33.000Z  1622980893416517632   en               False   \n",
       "1  2023-02-07T15:29:08.000Z  1622980790761013255   en               False   \n",
       "2  2023-02-07T15:29:08.000Z  1622980787946635264   de               False   \n",
       "3  2023-02-07T15:29:07.000Z  1622980784234840064   en               False   \n",
       "4  2023-02-07T15:29:05.000Z  1622980778622857216   en               False   \n",
       "\n",
       "             author_id reply_settings edit_history_tweet_ids  \\\n",
       "0  1547500137026551809       everyone  [1622980893416517632]   \n",
       "1  1015653575916126211       everyone  [1622980790761013255]   \n",
       "2  1356299130994388994       everyone  [1622980787946635264]   \n",
       "3           2989505299       everyone  [1622980784234840064]   \n",
       "4   859693365058617344       everyone  [1622980778622857216]   \n",
       "\n",
       "                                   referenced_tweets                   id  \\\n",
       "0  [{'type': 'retweeted', 'id': '1622902416868884...  1622980893416517632   \n",
       "1                                                NaN  1622980790761013255   \n",
       "2                                                NaN  1622980787946635264   \n",
       "3  [{'type': 'retweeted', 'id': '1622721798902251...  1622980784234840064   \n",
       "4  [{'type': 'retweeted', 'id': '1622721798902251...  1622980778622857216   \n",
       "\n",
       "                                                text  ...  \\\n",
       "0  RT @SaulStaniforth: \"I'm paid about £13 an hou...  ...   \n",
       "1  Trade: Brian Windhorst is sending $11.36 to a ...  ...   \n",
       "2  Falschparker Thread ist neue McDonalds Thread....  ...   \n",
       "3  RT @weirdlilguys: u got mcdonalds money? https...  ...   \n",
       "4  RT @weirdlilguys: u got mcdonalds money? https...  ...   \n",
       "\n",
       "                                entities.annotations  \\\n",
       "0                                                NaN   \n",
       "1  [{'start': 7, 'end': 21, 'probability': 0.8675...   \n",
       "2                                                NaN   \n",
       "3  [{'start': 24, 'end': 32, 'probability': 0.612...   \n",
       "4  [{'start': 24, 'end': 32, 'probability': 0.612...   \n",
       "\n",
       "                                       entities.urls   attachments.media_keys  \\\n",
       "0                                                NaN                      NaN   \n",
       "1                                                NaN                      NaN   \n",
       "2                                                NaN                      NaN   \n",
       "3  [{'start': 41, 'end': 64, 'url': 'https://t.co...  [3_1622464177188093952]   \n",
       "4  [{'start': 41, 'end': 64, 'url': 'https://t.co...  [3_1622464177188093952]   \n",
       "\n",
       "   in_reply_to_user_id  entities.cashtags  entities.hashtags  geo.place_id  \\\n",
       "0                  NaN                NaN                NaN           NaN   \n",
       "1                  NaN                NaN                NaN           NaN   \n",
       "2                  NaN                NaN                NaN           NaN   \n",
       "3                  NaN                NaN                NaN           NaN   \n",
       "4                  NaN                NaN                NaN           NaN   \n",
       "\n",
       "  attachments.poll_ids geo.coordinates.type geo.coordinates.coordinates  \n",
       "0                  NaN                  NaN                         NaN  \n",
       "1                  NaN                  NaN                         NaN  \n",
       "2                  NaN                  NaN                         NaN  \n",
       "3                  NaN                  NaN                         NaN  \n",
       "4                  NaN                  NaN                         NaN  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "pad_naar_bestand = \"/Users/jensvanasperen/GradedChallenge1/mcdonalds_or_@mcDonalds.jsonl\" \n",
    "\n",
    "all_tweets = []\n",
    "with open(pad_naar_bestand, 'r') as f:\n",
    "    for line in f:\n",
    "        line_data = json.loads(line)\n",
    "        if 'data' in line_data:\n",
    "            all_tweets.extend(line_data['data'])\n",
    "\n",
    "df = pd.json_normalize(all_tweets)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b1ca03-65df-4508-9c25-524481316abe",
   "metadata": {},
   "source": [
    "#1. How many cases are in your dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf31998c-fa71-4064-8e2a-bc60e5d943bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Total number of cases: 12331\n"
     ]
    }
   ],
   "source": [
    "print(f\"1. Total number of cases: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f120c12d-4fb9-42dd-947a-4a9887bc8db2",
   "metadata": {},
   "source": [
    "##To determine the dataset size, I used the len(df) function. This shows that the dataset contains 12,331 individual cases (tweets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd218750-e322-47f6-9841-1410330ade70",
   "metadata": {},
   "source": [
    "#2. What columns does the dataset contain and what is their data type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b848b2-7f5f-4bee-abb8-d93b5be440af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of data types:\n",
      "created_at                            str\n",
      "conversation_id                       str\n",
      "lang                                  str\n",
      "possibly_sensitive                   bool\n",
      "author_id                             str\n",
      "reply_settings                        str\n",
      "edit_history_tweet_ids             object\n",
      "referenced_tweets                  object\n",
      "id                                    str\n",
      "text                                  str\n",
      "public_metrics.retweet_count        int64\n",
      "public_metrics.reply_count          int64\n",
      "public_metrics.like_count           int64\n",
      "public_metrics.quote_count          int64\n",
      "public_metrics.impression_count     int64\n",
      "edit_controls.edits_remaining       int64\n",
      "edit_controls.is_edit_eligible       bool\n",
      "edit_controls.editable_until          str\n",
      "entities.mentions                  object\n",
      "context_annotations                object\n",
      "entities.annotations               object\n",
      "entities.urls                      object\n",
      "attachments.media_keys             object\n",
      "in_reply_to_user_id                   str\n",
      "entities.cashtags                  object\n",
      "entities.hashtags                  object\n",
      "geo.place_id                          str\n",
      "attachments.poll_ids               object\n",
      "geo.coordinates.type                  str\n",
      "geo.coordinates.coordinates        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Overview of data types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d35de29-f6f5-4407-979b-db5248d52e7e",
   "metadata": {},
   "source": [
    "##Inspected the data types using df.dtypes. The output shows a mix of different types. For example 'object' or 'bool'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ca200-3006-4c8a-8856-1ac36517ab23",
   "metadata": {},
   "source": [
    "#3. Are there any missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7508b80-929d-4d74-b9df-8c4112781ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "created_at                             0\n",
      "conversation_id                        0\n",
      "lang                                   0\n",
      "possibly_sensitive                     0\n",
      "author_id                              0\n",
      "reply_settings                         0\n",
      "edit_history_tweet_ids                 0\n",
      "referenced_tweets                   1527\n",
      "id                                     0\n",
      "text                                   0\n",
      "public_metrics.retweet_count           0\n",
      "public_metrics.reply_count             0\n",
      "public_metrics.like_count              0\n",
      "public_metrics.quote_count             0\n",
      "public_metrics.impression_count        0\n",
      "edit_controls.edits_remaining          0\n",
      "edit_controls.is_edit_eligible         0\n",
      "edit_controls.editable_until           0\n",
      "entities.mentions                   1386\n",
      "context_annotations                 1120\n",
      "entities.annotations                5832\n",
      "entities.urls                       5726\n",
      "attachments.media_keys              6674\n",
      "in_reply_to_user_id                 9528\n",
      "entities.cashtags                  12279\n",
      "entities.hashtags                  11585\n",
      "geo.place_id                       12231\n",
      "attachments.poll_ids               12322\n",
      "geo.coordinates.type               12325\n",
      "geo.coordinates.coordinates        12325\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values per column:\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33158bb5-6e63-4665-9520-9f0762d04126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total missing values in dataset: 104860\n"
     ]
    }
   ],
   "source": [
    "total_missing = df.isna().sum().sum()\n",
    "print(f\"\\nTotal missing values in dataset: {total_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc44be2-61cb-4636-97e5-bf3da8a4ec36",
   "metadata": {},
   "source": [
    "##Checked for missing values using isna().sum(). The output reveals that core columns like 'text' have 0 missing values, while optional fields like 'geo.place_id' have many.This confirms the dataset is complete enough for the required analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f61f8-f724-4f93-90a2-83e16432dea5",
   "metadata": {},
   "source": [
    "#4. What is the lowest number, highest number, average and the standard deviation of engagement (you can choose your metric)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1adb3b3a-b26c-406a-963b-a68a27d93bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Engagement Statistics (Likes):\n",
      "count    12331.000000\n",
      "mean         0.858892\n",
      "std         13.808035\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max       1160.000000\n",
      "Name: public_metrics.like_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n4. Engagement Statistics (Likes):\")\n",
    "stats = df['public_metrics.like_count'].describe()\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc6bb1f-9140-4673-baa3-ad083a24bb19",
   "metadata": {},
   "source": [
    "##Calculated the descriptive statistics for engagement.The output shows a mean of 0.86 and a maximum of 1,160 likes. This indicates that while most tweets have very low engagement, there are a few significant outliers in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b405dbf-509d-417e-99f7-42bb03d7aad8",
   "metadata": {},
   "source": [
    "#5. What is the most common language of the entries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e5be71-377f-48fc-9910-ba7a8dbae2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Most common language: en\n"
     ]
    }
   ],
   "source": [
    "common_lang = df['lang'].mode()[0]\n",
    "print(f\"\\n5. Most common language: {common_lang}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22da071b-935c-497d-a366-321d776b0379",
   "metadata": {},
   "source": [
    "##Identify the primary audience. The output is 'en', meaning English is the dominant language in this McDonald's dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3825d93-06a3-46a7-a8e9-4597b68fd0d3",
   "metadata": {},
   "source": [
    "#6. Who are the 5 most frequent authors in your dataset (can be poster, advertiser or subreddit)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a94a76f9-900f-4b4a-b361-9145cf32f5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Top 5 most frequent authors (IDs):\n",
      "author_id\n",
      "60421964               110\n",
      "71026122                70\n",
      "1470764537418862594     30\n",
      "2388019676              22\n",
      "1343141573622718466     19\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n6. Top 5 most frequent authors (IDs):\")\n",
    "print(df['author_id'].value_counts().head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f717b46-42d5-483e-9567-eb4cc88932ab",
   "metadata": {},
   "source": [
    "##Identify the top 5 most active authors by their ID. The output lists the top 5 Author IDs, with the most frequent author appearing 110 times. This identifies the most prominent voices in the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d75e3e-cf1a-4348-afef-9a1fe5f350b1",
   "metadata": {},
   "source": [
    "#7. Write a function that categorizes the content of your dataset based on information available in one of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd8ddf1-dda1-4742-aab5-f69cb94bcf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_engagement(likes):\n",
    "    \"\"\"\n",
    "    This function categorizes a tweet based on the number of likes.\n",
    "    1 = High engagement (more than 10 likes)\n",
    "    0 = Standard engagement (10 likes or less)\n",
    "    \"\"\"\n",
    "    if likes > 10:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a010e96c-e2fc-4e5a-b454-18f267044ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'public_metrics.like_count' column \n",
    "# and create a new column called 'engagement_labeldf['engagement_label'] = df['public_metrics.like_count'].apply(categorize_engagement)\n",
    "df['engagement_label'] = df['public_metrics.like_count'].apply(categorize_engagement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21c1aedb-5e6f-47fc-a71c-a585dcbb34f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of the categorization:\n",
      "   public_metrics.like_count  engagement_label\n",
      "0                          0                 0\n",
      "1                          1                 0\n",
      "2                          1                 0\n",
      "3                          0                 0\n",
      "4                          0                 0\n",
      "5                          0                 0\n",
      "6                          0                 0\n",
      "7                          0                 0\n",
      "8                          0                 0\n",
      "9                          0                 0\n"
     ]
    }
   ],
   "source": [
    "# Show the first 10 rows to see the new column\n",
    "print(\"Result of the categorization:\")\n",
    "print(df[['public_metrics.like_count', 'engagement_label']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3282f0a5-3f36-486c-b6ac-1f408cb48c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value counts of the new categories:\n",
      "engagement_label\n",
      "0    12187\n",
      "1      144\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count how many tweets fall into each category\n",
    "print(\"\\nValue counts of the new categories:\")\n",
    "print(df['engagement_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccdd85b-744a-4685-8682-76b4311e33ca",
   "metadata": {},
   "source": [
    "##Wrote a custom Python function to categorize tweets into 'High Engagement' (1) or 'Standard' (0) based on a threshold of 10 likes. There are 12187 'Standard' tweets and 144 'High Engagement' tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2e7676-4e57-4815-94a9-1c15213696a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-presentation",
   "metadata": {},
   "source": [
    "## 2. Intepretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8fd49e-0e9d-4adb-8047-af7bdcfd48e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aging-literacy",
   "metadata": {},
   "source": [
    "### Explaining your process\n",
    "\n",
    "Write a brief description explaining how you created or assembled the code (i.e., sources used, and how did you adjust the code). In this description, include a brief reflection of the main challenges you had during the process, and how you handled these challenges.\n",
    "\n",
    "*Wordcount: maximum of 200 words. Please inform the wordcount for this answer*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "711fd5ac",
   "metadata": {},
   "source": [
    "To assemble the code, I followed the CRISP-DM framework focusing on the Data Understanding and Preparation phases. I used the Pandas library to handle the McDonald's dataset, specifically using pd.json_normalize to flatten the nested JSONL structure into a readable table. This allowed me to access columns like public_metrics.like_count for statistical analysis.\n",
    "\n",
    "The main challenge was the complex structure of the JSONL data, where essential metrics like \"like_count\" were nested within a dictionary called \"public_metrics\". Initially, Pandas could not perform calculations because it saw the entire dictionary as a single object. I handled this by applying a normalization technique to get the data into individual columns. Another challenge was verifying data quality; I had to learn that Pandas labels text as 'object' for example. Finally, I adjusted the code for question 7 by creating a custom 'if-else' function to categorize engagement, ensuring the data was correctly prepared for interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730734e8-49dc-47ef-90eb-4298cc366cb9",
   "metadata": {},
   "source": [
    "Wordcount: 148"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-piece",
   "metadata": {},
   "source": [
    "### Explaining your code\n",
    "\n",
    "You used a lot of code above to answer the seven questions. Select what you consider to be the three most important steps in the code, justify why they are important, and explain the commands being used. \n",
    "\n",
    "*Note: there are different correct answers to what the three most important steps of the code are. We want to understand your reasoning/justification for this, and will accept different answers depending on the logic and clarity of your argumentation*.\n",
    "\n",
    "*Wordcount: maximum of 200 words. Please inform the wordcount for this answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c90f84",
   "metadata": {},
   "source": [
    "Data Normalization: This was the first essential step in the process. Since the social media data was structured in complex layers, the information was flattened into a standard table format. This process is crucial because it makes hidden information, such as engagement metrics, accessible for analysis.\n",
    "\n",
    "Descriptive Statistical Analysis: This step provided a simple view of the dataset during the exploration phase. By calculating values like the mean and maximum, it was possible to understand the typical reach of a tweet and identify significant outliers in the McDonald's data.\n",
    "\n",
    "Applying a Categorization Logic: A specific method was used to transform raw numbers into meaningful categories. This is a vital part of preparing the data, as it allows for a clear distinction between standard interactions and high-impact content. This makes the final insights much more actionable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16af4eb-019a-4a9d-94a4-2a6637cd6b1f",
   "metadata": {},
   "source": [
    "Wordcount: 135"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-management",
   "metadata": {},
   "source": [
    "### Quality assurance\n",
    "\n",
    "You are assembling code from different sources - some of it you learned in the tutorial, you may have reused code we provided, and in some cases you may have used different online sources. But how do you know that the code actually worked and delivered the expected results? Please explain how someone else can verify that the code worked (e.g., what should they look at) and which steps you built into the code for yourself to know that it worked (explicitly indicate at least three steps)\n",
    "\n",
    "*Wordcount: maximum of 200 words. Please inform the wordcount for this answer*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c075a17",
   "metadata": {},
   "source": [
    "To guarantee that the code functioned correctly and delivered the expected results, several validation steps were integrated throughout the process. An external reviewer can verify the accuracy of the findings by inspecting the printed outputs directly beneath each code cell, which provide visual confirmation of the data transformations.\n",
    "\n",
    "Specifically, three primary verification mechanisms were built into the workflow to monitor performance. First, a row count verification was performed immediately after loading the dataset to confirm that the entire file was imported without data loss. Second, quality checks were executed on data types and missing values to ensure that all calculations were based on clean, numerically correct data. Finally, a visual sampling was conducted by displaying the first five rows of the modified table and generating frequency counts for the new categories. This allowed for a manual check to ensure that the categorization logic was applied as intended to the final dataset. These combined measures ensure that the final output is reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a65620-7183-40db-a18d-d00934a704a9",
   "metadata": {},
   "source": [
    "Wordcount: 161 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
